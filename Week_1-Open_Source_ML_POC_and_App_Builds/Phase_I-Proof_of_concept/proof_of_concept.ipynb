{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align = \"center\" draggable=‚Äùfalse‚Äù ><img src=\"https://user-images.githubusercontent.com/37101144/161836199-fdb0219d-0361-4988-bf26-48b0fad160a3.png\"\n",
    "     width=\"200px\"\n",
    "     height=\"auto\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "# <h1 align=\"center\" id=\"heading\">Phase I - Proof of Concept</h1>\n",
    "\n",
    "\n",
    "\n",
    "## ‚òëÔ∏è Objectives\n",
    "At the end of this session, you will have a brief understanding of how to:\n",
    "- [ ] Find and run pre-trained models (Phase I)\n",
    "- [ ] Evaluate results from pre-trained models (Phase I)\n",
    "- [ ] Run a pre-trained model using real Reddit data (Phase I)\n",
    "\n",
    "\n",
    "## üõ†Ô∏è Pre-Assignment\n",
    "1. Create a virtual environment with üêç conda : `conda env create -f environment.yml`\n",
    "\n",
    "2. Activate your conda virtual environment: `conda activate tsla_bot`\n",
    "\n",
    "3. Create a .env file in the root directory and add the following variables:\n",
    "4. \n",
    "   `STOCK_API_KEY` : API key from [twelvedata](https://twelvedata.com/pricing)\n",
    "\n",
    "   `REDDIT_API_CLIENT_ID` : client ID of your reddit app\n",
    "   \n",
    "   `REDDIT_API_CLIENT_SECRET`: client secret of your reddit app\n",
    "   \n",
    "   Follow this tutorial to generate your own Reddit credentials:\n",
    "   <https://www.jcchouinard.com/get-reddit-api-credentials-with-praw/>\n",
    "\n",
    "4. Continue in this notebook\n",
    "\n",
    "\n",
    "\n",
    "## Background\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the meeting with your boss, and reviewing your [notes](https://www.notion.so/Analyzing-Market-Sentiment-Phase-I-II-and-II-End-to-End-MLOps-with-Open-Source-Tools-dc4b846108b44f6bb2962d550368560c#54cc350bc95041ee873dabde36930af1) üìì, you're ready to get going on a Proof of Concept (POC)\n",
    "\n",
    "A POC tests the validity of your hypothesis. It's a way to prove that your idea, task, app, or whatever else works!\n",
    "\n",
    "There's no time to waste - you've got an idea - it's time to get testing it out! üèÅ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Initial Imports and Variable Setting üìà\n",
    "\n",
    "First things first: Let's set some variables that will help us going forward.\n",
    "\n",
    "Though your boss suggested `\"TSLA\"`, you can use any active stock-symbol and subreddit!\n",
    "\n",
    "**IMPORTANT**: Make sure you verify that your selected subreddit exists by navigating to the generated link after you run the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "### START CODE HERE\n",
    "\n",
    "# Stock data to grab ex. \"TSLA\"\n",
    "symbol = \n",
    "\n",
    "# subreddit to check\n",
    "subreddit = \n",
    "\n",
    "# Time interval granularity\n",
    "# valid choices are \"1week\", \"1month\", \"1day\"\n",
    "interval = \n",
    "\n",
    "# set the beginning and end of the time range you'd like to analyze\n",
    "# ensure you use the format \"YYYY-MM-DD\"\n",
    "start_date = \n",
    "end_date = \n",
    "\n",
    "### END CODE HERE\n",
    "\n",
    "start_date_dt = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "end_date_dt = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "\n",
    "print(f'reddit.com/r/{subreddit}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make sure we can import our `BotUtils.py`, we have to ensure we're running from the `TSLASentimentAnalyzer` folder. To do this, we can `cd` into that directory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd TSLASentimentAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Set Environment Variables ‚õ∞Ô∏è\n",
    "\n",
    "Use the information you created in the instructions of the `README.md` to fill in the values below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define env variables for configuration\n",
    "import os\n",
    "os.environ['REDDIT_API_CLIENT_ID'] = \"\"\n",
    "os.environ['REDDIT_API_CLIENT_SECRET'] = \"\"\n",
    "os.environ['STOCK_DATA_API_KEY'] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Get to Scraping üåê\n",
    "\n",
    "Now we're going to use the `scraper` module (found in the `TSLASentimentAnalyzer` folder) to \"scrape\" the subreddit of our choosing for posts!\n",
    "\n",
    "‚öóÔ∏è RESOURCES: \n",
    "\n",
    "[Web Scraping](https://www.parsehub.com/blog/what-is-web-scraping/)\n",
    "\n",
    "[Reddit Post Options](https://www.reddit.com/r/help/comments/32eu8w/what_is_the_difference_between_newrising_hot_top/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initializing the Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from loguru import logger\n",
    "from TSLASentimentAnalyzer.classifier import predict\n",
    "from TSLASentimentAnalyzer.scraper import RedditScraper\n",
    "from TSLASentimentAnalyzer.config import settings\n",
    "\n",
    "# instantiating the reddit scraper\n",
    "reddit = ### YOUR LINE OF CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Helper Functions üî®\n",
    "\n",
    "Here are some helper functions to assist in collecting data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(number: int, scraping_option: str):\n",
    "    '''\n",
    "    loads comments from reddit using the RedditScraper using one of the options\n",
    "    and returns a DataFrame\n",
    "    '''\n",
    "    comments = []\n",
    "    for submission in scraping_option(number):\n",
    "        comments.extend(reddit.get_comment_forest(submission.comments))\n",
    "        logger.debug(\n",
    "            submission.title,\n",
    "            submission.num_comments,\n",
    "            len(reddit.get_comment_forest(submission.comments)),\n",
    "        )\n",
    "    df = pd.DataFrame(comments)\n",
    "    return df\n",
    "\n",
    "\n",
    "def select_scrap_type(option: str):\n",
    "    '''\n",
    "    selects a method from the reddit object based on a given option\n",
    "    '''\n",
    "    if option == \"Hot\":\n",
    "        return reddit.get_hot\n",
    "        \n",
    "    if option == \"Rising\":\n",
    "        return reddit.get_rising\n",
    "\n",
    "    if option == \"New\":\n",
    "        return reddit.get_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading & Processing the Reddit Comment Data\n",
    "\n",
    "Let's scrape the 15 üî• hottest üî• posts from your selected subreddit using the `load_data()` helper function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping data from reddit\n",
    "dfReddit = ### YOUR LINE OF CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our comments, let's use the [`pandas.Series.str.slice()`](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.slice.html) method to ensure our data will play nicely with our model. (`bert` only supports up to a maximum of 512 tokens, so a range of `0` to `512` seems appropriate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice comments as bert supports only 512 tokens\n",
    "dfReddit['comment'] = ### YOUR LINE OF CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Analyze the Comments! üîç\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using the `FourthBrain/bert_model_reddit_tsla` model (found [here](https://huggingface.co/FourthBrain/bert_model_reddit_tsla)), which is based off of the `distilbert-base-uncased` (found [here](https://huggingface.co/distilbert-base-uncased))\n",
    "\n",
    "We'll use this model in a `sentiment-analysis` pipeline! (read all about that [here](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.pipeline.example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# initialize the sentiment pipeline\n",
    "sentiment_pipeline = ### YOUR LINE OF CODE HERE\n",
    "\n",
    "\n",
    "reddit_json = sentiment_pipeline(dfReddit[\"comment\"].tolist())\n",
    "\n",
    "# Retrieve labels and scores\n",
    "dfReddit['label'] = [reddit_json[i]['label'] for i in range(0, len(reddit_json))]\n",
    "dfReddit['score'] = [reddit_json[i]['score'] for i in range(0, len(reddit_json))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, we'll want to perform the following steps\n",
    "\n",
    "1. Convert the `created_at` column to the appropriate format and name it `'timestamp'`\n",
    "2. Normalize the dates using the `NormalizeDates()` helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from BotUtils import NormalizeDates\n",
    "\n",
    "# Rename timestamp column \n",
    "dfReddit = dfReddit.rename(columns={'created_at': 'timestamp'})\n",
    "dfReddit['timestamp'] = [datetime.fromtimestamp(dt) for dt in dfReddit['timestamp'] ]\n",
    "\n",
    "# Normalize Reddit sentiment data\n",
    "dfReddit = NormalizeDates(dfReddit, timestamp_col=\"timestamp\", interval=interval)\n",
    "\n",
    "# Create a continous time series \n",
    "dfTimeSeries = pd.DataFrame(dfReddit[\"timestamp\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Get Stock Data üöÄ\n",
    "\n",
    "Next up, we're going to get the stock data and create a DataFrame - as well as normalize the dates, just like we did with the subreddit comment timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BotUtils import GetStockData\n",
    "\n",
    "# Get Stock Data\n",
    "j, dfStockData = GetStockData(settings.stock_data_api_key, symbol=symbol, start_date=start_date, end_date=end_date, interval='1day')\n",
    "dfStockData = dfStockData[(dfStockData['timestamp'] >= start_date) & (dfStockData['timestamp'] <= end_date)]\n",
    "\n",
    "# Normalize stock price data\n",
    "dfStockData = NormalizeDates(dfStockData, timestamp_col=\"timestamp\", interval=interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Merge Sentiment and Stock Data üß¨\n",
    "\n",
    "We've got some processing to do! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join time series df with stock price df and reddit df\n",
    "dfSentiment = dfTimeSeries.merge(dfStockData, how='left', on=\"timestamp\")\n",
    "dfSentiment = dfSentiment.merge(dfReddit, how='left', on=\"timestamp\")\n",
    "\n",
    "# Filter down to only the columns that we'll be using \n",
    "dfSentiment = dfSentiment[['timestamp', 'close', 'volume', 'label', 'score']]\n",
    "\n",
    "# Clean up NaNs for closing price, score, and labels\n",
    "dfSentiment['close'] = dfSentiment['close'].fillna(0)\n",
    "dfSentiment['score'] = dfSentiment['score'].fillna(0)\n",
    "dfSentiment['label'] = dfSentiment['label'].fillna('NEUTRAL')\n",
    "dfSentiment['label'] = ['NEGATIVE' for s in dfSentiment['label'] == 'LABEL_0']\n",
    "dfSentiment['label'] = ['POSITIVE' for s in dfSentiment['label'] == 'LABEL_1']\n",
    "\n",
    "# Convert close from string to float\n",
    "dfSentiment['close'] = dfSentiment['close'].astype('float')\n",
    "\n",
    "# Calculate weighted sentiment\n",
    "dfSentiment['sentiment'] = [1 if sentiment == \"POSITIVE\"  else 0 if sentiment == \"NEUTRAL\" else -1 for sentiment in dfSentiment['label'].tolist() ]\n",
    "dfSentiment['weighted_sentiment'] = dfSentiment['sentiment'] * dfSentiment['score']\n",
    "\n",
    "# Count only the POSITIVE and NEGATIVE labels (NEUTRAL is just a filler for missing dates)\n",
    "dfSentiment['counter'] = [1 if sentiment == \"POSITIVE\"  else 1 if sentiment == \"NEGATIVE\" else 0 for sentiment in dfSentiment['label'].tolist() ]\n",
    "\n",
    "# Group by to calculate Reddit post count and sentiment score (mean of weighted sentiment)\n",
    "dfSentiment = dfSentiment.groupby('timestamp') \\\n",
    "       .agg({'counter':'sum', 'close':'max', 'volume':max, 'weighted_sentiment':'mean'}) \\\n",
    "       .rename(columns={'sentiment':'count_posts', 'weighted_sentiment':'sentiment_score'}) \\\n",
    "       .reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Final Computation üñ•Ô∏è\n",
    "\n",
    "In this last step, we're going to finish up some calculations:\n",
    "\n",
    "1. Create the lags and percentage change for the closing stock price. \n",
    "2. Calculate the 3 month rolling average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lags and %change for closing stock price\n",
    "dfSentiment['close_lag1'] = dfSentiment['close'].shift(1)\n",
    "dfSentiment['close_lag1'] = dfSentiment['close_lag1'].fillna(0)\n",
    "dfSentiment['perc_change_close'] = (dfSentiment['close'] - dfSentiment['close_lag1']) / dfSentiment['close_lag1']\n",
    "dfSentiment['perc_change_close'] = dfSentiment['perc_change_close'].fillna(0)\n",
    "\n",
    "dfSentiment['sentiment_score_lag1'] = dfSentiment['sentiment_score'].shift(1)\n",
    "dfSentiment['sentiment_score_lag1'] = dfSentiment['sentiment_score_lag1'].fillna(0)\n",
    "dfSentiment['perc_change_sentiment'] = (dfSentiment['sentiment_score'] - dfSentiment['sentiment_score_lag1']) / dfSentiment['sentiment_score_lag1']\n",
    "\n",
    "# Calculate 3 month rolling average\n",
    "dfSentiment['sentiment_SMA3mo'] = dfSentiment.sentiment_score.rolling(3).mean()\n",
    "dfSentiment['sentiment_SMA3mo'] = dfSentiment['sentiment_SMA3mo'].fillna(0)\n",
    "\n",
    "dfSentiment = dfSentiment[1:]\n",
    "dfSentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save our work in a `.csv` for use later in the Streamlit app!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame locally (or somewhere else) for use in Streamlit app\n",
    "dfSentiment.to_csv('./sentiment_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8a: Plotting our Results! üìä\n",
    "\n",
    "Now, after all that, let's plot our results and see how they shape up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ax1 = dfSentiment.plot(kind = 'line', x = 'timestamp',\n",
    "                  y = 'sentiment_score', color = 'Red',\n",
    "                  linewidth = 3)\n",
    "\n",
    "ax2 = dfSentiment.plot(kind = 'line', x = 'timestamp',\n",
    "                   y = 'close', secondary_y = True,\n",
    "                   color = 'Blue',  linewidth = 3,\n",
    "                   ax = ax1) \n",
    "\n",
    "#labeling x and y-axis\n",
    "ax1.set_xlabel('Timestamp', color = 'black')\n",
    "ax1.set_ylabel('Sentiment Score', color = \"r\")\n",
    "ax2.set_ylabel('% Change Stock Price', color = 'b')\n",
    " \n",
    "#defining display layout\n",
    "plt.tight_layout()\n",
    " \n",
    "#show plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "1f13e323035dff4717f5511b157fe27444f57a85a8aba8c50e4e514a3b6c5f87"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
